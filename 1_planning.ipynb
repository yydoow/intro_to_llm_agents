{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Load configuration\n",
    "config = yaml.safe_load(open(\"myconfig.yml\"))\n",
    "\n",
    "# Set up Azure OpenAI environment variables with the minimum necessary configuration\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = config[\"AZURE_OPENAI_API_KEY\"]\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = config[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = config[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-05-15\"  # Using a stable API version that works with most Azure OpenAI deployments\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = config[\"TAVILY_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = str(config[\"LANGCHAIN_TRACING_V2\"]).lower()\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = config[\"LANGCHAIN_ENDPOINT\"]\n",
    "os.environ[\"LANGCHAIN_HUB_API_URL\"] = config[\"LANGCHAIN_HUB_API_URL\"]\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = str(config[\"LANGCHAIN_WANDB_TRACING\"]).lower()\n",
    "os.environ[\"WANDB_PROJECT\"] = str(config[\"WANDB_PROJECT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning\n",
    "\n",
    "![images/Screenshot 2024-02-21 at 16.17.22.png](<images/Screenshot 2024-02-21 at 16.17.22.png>)\n",
    "\n",
    "You might have come across various techniques aimed at improving the performance of large language models, such as offering tips or even jokingly threatening them. One popular technique is called \"chain of thought,\" where the model is asked to think step by step, enabling self-correction. This approach has evolved into more advanced versions like \"chain of thought with self-consistency\" and the generalized \"tree of thoughts,\" where multiple thoughts are created, re-evaluated, and consolidated to provide an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree of Thoughts\n",
    "\n",
    "The [@astropomeai tutorial](https://medium.com/@astropomeai/implementing-the-tree-of-thoughts-in-langchains-chain-f2ebc5864fac) on Tree of Thoughts is used as basis of this exercise but expanded with LLMOps tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "cot_step1 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step1\")\n",
    "cot_step2 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step2\")\n",
    "cot_step3 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step3\")\n",
    "cot_step4 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import AzureChatOpenAI\n",
    "\n",
    "# Get Azure OpenAI configuration from environment variables\n",
    "deployment_name = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "api_version = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "\n",
    "azure_llm = AzureChatOpenAI(\n",
    "    deployment_name=deployment_name,\n",
    "    api_version=api_version,\n",
    "    api_key=api_key,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=azure_llm,\n",
    "    prompt=cot_step1,\n",
    "    output_key=\"solutions\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=azure_llm,\n",
    "    prompt=cot_step2,\n",
    "    output_key=\"review\"\n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=azure_llm,\n",
    "    prompt=cot_step3,\n",
    "    output_key=\"deepen_thought_process\"\n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=azure_llm,\n",
    "    prompt=cot_step4,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'human colonization of Mars', 'perfect_factors': 'The distance between Earth and Mars is very large, making regular resupply difficult', 'ranked_solutions': \"### **Ranking of Solutions**\\n\\n1. **Solution 2: Modular Supply Chain with Orbital Depots**  \\n   **Justification:**  \\n   This solution ranks highest due to its practicality, scalability, and ability to address immediate needs for a Martian colony. Establishing a reliable supply chain ensures that colonists have access to essential resources (food, water, equipment) during the early stages of settlement, when self-sufficiency is not yet achievable. The modular nature of the depots allows for incremental expansion and adaptation to growing demands. Additionally, this approach leverages existing technologies (e.g., reusable spacecraft) and partnerships with aerospace companies, making it the most feasible in the short term.  \\n   **Final Thoughts:**  \\n   While this solution is critical for the colony's initial survival, over-reliance on Earth-based resupply could hinder progress toward self-sufficiency. It should be viewed as a transitional strategy rather than a permanent solution.\\n\\n2. **Solution 1: Self-Sustaining Martian Ecosystem**  \\n   **Justification:**  \\n   This solution ranks second due to its long-term potential to create a truly independent Martian colony. A self-sustaining ecosystem would eliminate reliance on Earth, reduce costs, and enable population growth. However, the technical challenges of developing and maintaining a closed-loop system in the harsh Martian environment are significant. Early failures could jeopardize the colony's survival, making this solution less reliable in the short term.  \\n   **Final Thoughts:**  \\n   This solution should be pursued alongside the modular supply chain, with initial testing and scaling conducted in controlled environments. Success in this area would be transformative, not only for Mars but also for sustainable practices on Earth.\\n\\n3. **Solution 3: Genetic and Technological Adaptation of Humans**  \\n   **Justification:**  \\n   This solution ranks third due to its ethical complexities, societal resistance, and long development timelines. While biomedical and technological advancements could improve human resilience to Martian conditions, the risks associated with genetic modifications and the slow pace of terraforming make this solution less practical in the near term. Additionally, the focus on human adaptation may divert resources from more immediate priorities, such as infrastructure and resource management.  \\n   **Final Thoughts:**  \\n   This solution should be approached cautiously, with an emphasis on non-invasive technologies (e.g., wearable devices, robotics) and incremental biomedical improvements. Genetic engineering and terraforming should remain long-term goals, pursued only after the colony is stable and ethical concerns are thoroughly addressed.\\n\\n---\\n\\n### **Integrated Approach Recommendation**\\n\\nAn integrated strategy combining elements of all three solutions is the most promising path forward:\\n\\n1. **Short-Term (0–10 years):**  \\n   Focus on establishing the modular supply chain to ensure reliable resupply and support for the colony's initial infrastructure. Begin testing small-scale self-sustaining ecosystems in controlled environments on Earth and Mars.\\n\\n2. **Mid-Term (10–30 years):**  \\n   Gradually scale up self-sustaining ecosystems to reduce dependence on Earth. Introduce biomedical and robotic technologies to enhance human resilience and productivity in the Martian environment.\\n\\n3. **Long-Term (30+ years):**  \\n   Transition to full self-sufficiency through advanced ecosystem technologies and localized terraforming efforts. Explore genetic engineering cautiously, prioritizing safety and ethical considerations.\\n\\n---\\n\\n### **Final Considerations**\\n\\n- **Risk Mitigation:** Each solution carries unique risks, from technical failures to ethical dilemmas. A diversified approach reduces the impact of any single failure and ensures progress across multiple fronts.\\n- **Collaboration:** Success will require unprecedented collaboration between governments, private companies, research institutions, and international organizations. Clear communication and shared goals are essential.\\n- **Flexibility:** The Martian environment is unpredictable, and plans must remain adaptable to unforeseen challenges and opportunities.\\n- **Earth Benefits:** Many technologies developed for Mars (e.g., closed-loop ecosystems, biomedical advancements) could have significant applications on Earth, addressing global challenges like food security and climate change.\\n\\nBy leveraging the strengths of each solution and addressing their weaknesses, humanity can establish a sustainable Martian colony that serves as a stepping stone for further exploration and innovation.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# Set up the chain with the Azure OpenAI LLM\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\", \"perfect_factors\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\n",
    "        overall_chain.invoke(\n",
    "            {\n",
    "                \"input\": \"human colonization of Mars\",\n",
    "                \"perfect_factors\": \"The distance between Earth and Mars is very large, making regular resupply difficult\"\n",
    "            }\n",
    "    )\n",
    ")\n",
    "\n",
    "#print(\n",
    "#        overall_chain.invoke(\n",
    "#            {\n",
    "#                \"input\": \"Father teaching his son how to play chess\",\n",
    "#                \"perfect_factors\": \"The father is a SRE, while the son is a 8 year old child\"\n",
    "#            }\n",
    "#        )\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct prompt overview\n",
    "\n",
    "Let's review [ReAct](https://python.langchain.com/docs/modules/agents/agent_types/react) prompt as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-ask with search\n",
    "\n",
    "Let's review [self-ask with search](https://python.langchain.com/docs/modules/agents/agent_types/self_ask_with_search) as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Who lived longer, Muhammad Ali or Alan Turing?\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali\\n\\nQuestion: When was the founder of craigslist born?\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the founder of craigslist?\\nIntermediate answer: Craigslist was founded by Craig Newmark.\\nFollow up: When was Craig Newmark born?\\nIntermediate answer: Craig Newmark was born on December 6, 1952.\\nSo the final answer is: December 6, 1952\\n\\nQuestion: Who was the maternal grandfather of George Washington?\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball\\n\\nQuestion: Are both the directors of Jaws and Casino Royale from the same country?\\nAre follow up questions needed here: Yes.\\nFollow up: Who is the director of Jaws?\\nIntermediate answer: The director of Jaws is Steven Spielberg.\\nFollow up: Where is Steven Spielberg from?\\nIntermediate answer: The United States.\\nFollow up: Who is the director of Casino Royale?\\nIntermediate answer: The director of Casino Royale is Martin Campbell.\\nFollow up: Where is Martin Campbell from?\\nIntermediate answer: New Zealand.\\nSo the final answer is: No\\n\\nQuestion: {input}\\nAre followup questions needed here:{agent_scratchpad}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaches to read next:\n",
    "\n",
    "**Reflexion** ([Shinn & Labash 2023](https://arxiv.org/abs/2303.11366)) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills.\n",
    "\n",
    "**Chain of Hindsight** (CoH; [Liu et al. 2023](https://arxiv.org/abs/2302.02676)) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
